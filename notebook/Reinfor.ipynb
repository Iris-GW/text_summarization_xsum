{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from datasets import load_dataset\n",
    "from rouge import Rouge\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import wandb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the XSum dataset\n",
    "dataset = load_dataset(\"xsum\")\n",
    "\n",
    "# Reduce dataset size\n",
    "train_indices, _ = train_test_split(range(len(dataset['train'])), test_size=0.5, random_state=42)\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project=\"text_summarization\", name=\"reinforce_summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "def preprocess_batch(batch):\n",
    "    input_texts = [\"summarize: \" + doc for doc in batch[\"document\"]]\n",
    "    target_texts = batch[\"summary\"]\n",
    "\n",
    "    source = tokenizer(input_texts, max_length=512, truncation=True, padding='max_length', return_tensors=\"pt\")\n",
    "    target = tokenizer(target_texts, max_length=150, truncation=True, padding='max_length', return_tensors=\"pt\")\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": source[\"input_ids\"].tolist(),\n",
    "        \"attention_mask\": source[\"attention_mask\"].tolist(),\n",
    "        \"labels\": target[\"input_ids\"].tolist(),\n",
    "    }\n",
    "\n",
    "# Replace the original train dataset with the reduced dataset\n",
    "reduced_dataset = {}\n",
    "reduced_dataset[\"train\"] = dataset[\"train\"].select(train_indices)\n",
    "reduced_dataset[\"validation\"] = dataset[\"validation\"]\n",
    "reduced_dataset[\"test\"] = dataset[\"test\"]\n",
    "\n",
    "# Preprocess the data\n",
    "tokenized_dataset = {}\n",
    "for split in reduced_dataset:\n",
    "    tokenized_dataset[split] = reduced_dataset[split].map(preprocess_batch, remove_columns=[\"document\", \"summary\"], batched=True, batch_size=16)\n",
    "\n",
    "train_dataset = tokenized_dataset[\"train\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha = 0.9\n",
    "gamma = 0.99\n",
    "n_epochs = 3\n",
    "batch_size = 8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "rouge = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Rouge-L score\n",
    "def compute_rouge_l(pred_summary, ref_summary):\n",
    "    scores = rouge.get_scores(pred_summary, ref_summary)\n",
    "    return scores[0][\"rouge-l\"][\"f\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenized_dataset):\n",
    "        self.tokenized_dataset = tokenized_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            \"input_ids\": torch.tensor(self.tokenized_dataset[\"input_ids\"][idx]),\n",
    "            \"attention_mask\": torch.tensor(self.tokenized_dataset[\"attention_mask\"][idx]),\n",
    "            \"labels\": torch.tensor(self.tokenized_dataset[\"labels\"][idx]),\n",
    "        }\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CustomDataset(tokenized_dataset[\"train\"])\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Log hyperparameters\n",
    "config = wandb.config\n",
    "config.learning_rate = 1e-5\n",
    "config.batch_size = batch_size\n",
    "config.n_epochs = n_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom collate function\n",
    "def collate_fn(batch):\n",
    "    keys = batch[0].keys()\n",
    "    batch_dict = {key: torch.stack([item[key] for item in batch]) for key in keys}\n",
    "    return batch_dict\n",
    "\n",
    "# Update the DataLoader with the custom collate function\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train dataset size:\", len(reduced_dataset[\"train\"]))\n",
    "print(\"Validation dataset size:\", len(reduced_dataset[\"validation\"]))\n",
    "print(\"Test dataset size:\", len(reduced_dataset[\"test\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tokenized train dataset size:\", len(tokenized_dataset[\"train\"]))\n",
    "print(\"Tokenized validation dataset size:\", len(tokenized_dataset[\"validation\"]))\n",
    "print(\"Tokenized test dataset size:\", len(tokenized_dataset[\"test\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_epoch_batches = math.ceil(len(train_dataloader) / 2)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{n_epochs}\")\n",
    "    for i, batch in enumerate(progress_bar):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Generate summaries\n",
    "        with torch.no_grad():\n",
    "            summary_ids = model.generate(input_ids, attention_mask=attention_mask, num_beams=4, max_length=150, early_stopping=True)\n",
    "        pred_summaries = [tokenizer.decode(s, skip_special_tokens=True) for s in summary_ids]\n",
    "        ref_summaries = [tokenizer.decode(s, skip_special_tokens=True) for s in labels]\n",
    "\n",
    "        # Calculate rewards\n",
    "        rewards = []\n",
    "        for pred_summary, ref_summary in zip(pred_summaries, ref_summaries):\n",
    "            reward = compute_rouge_l(pred_summary, ref_summary)\n",
    "            rewards.append(reward)\n",
    "\n",
    "        rewards = torch.tensor(rewards).to(device)\n",
    "\n",
    "        # Pad summary_ids to match the sequence length of logits\n",
    "        logits = model(input_ids, attention_mask=attention_mask, decoder_input_ids=summary_ids).logits\n",
    "        max_seq_len = logits.size(1)\n",
    "        padded_summary_ids = torch.zeros(batch_size, max_seq_len).long().to(device)\n",
    "        padded_summary_ids[:, :summary_ids.size(1)] = summary_ids\n",
    "\n",
    "        # Compute the policy gradient loss\n",
    "        log_probs = torch.gather(logits.view(-1, logits.size(-1)), 1, padded_summary_ids.view(-1, 1)).view(batch_size, -1)\n",
    "        loss = -(rewards.view(-1, 1) * log_probs).sum() / batch_size\n",
    "\n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"batch_loss\": loss.item()})\n",
    "\n",
    "         # Save checkpoint every 0.5 epoch\n",
    "        if (i + 1) == half_epoch_batches or (i + 1) == len(train_dataloader):\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"model_epoch_{epoch + 1}_batch_{i + 1}.pt\")\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Checkpoint saved at: {checkpoint_path}\")\n",
    "\n",
    "        # Update the progress bar\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{n_epochs} | Loss: {avg_epoch_loss}\")\n",
    "\n",
    "    # Log the average loss for the current epoch\n",
    "    wandb.log({\"epoch\": epoch + 1, \"loss\": avg_epoch_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "def generate_summary(text):\n",
    "    input_text = \"summarize: \" + text\n",
    "    inputs = tokenizer(input_text, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        summary_ids = model.generate(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], num_beams=4, max_length=150, early_stopping=True)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "rouge = Rouge()\n",
    "test_dataset = tokenized_dataset[\"test\"]\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for example in test_dataset:\n",
    "    article = tokenizer.decode(example[\"input_ids\"], skip_special_tokens=True)\n",
    "    pred_summary = generate_summary(article)\n",
    "    ref_summary = tokenizer.decode(example[\"labels\"], skip_special_tokens=True)\n",
    "\n",
    "    predictions.append(pred_summary)\n",
    "    references.append(ref_summary)\n",
    "\n",
    "rouge_scores = rouge.compute(predictions=predictions, references=references, rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "\n",
    "print(\"Rouge Scores:\", rouge_scores)\n",
    "\n",
    "# Calculate BLEU scores\n",
    "smooth = SmoothingFunction().method1\n",
    "bleu_scores = [sentence_bleu([ref.split()], pred.split(), smoothing_function=smooth) for ref, pred in zip(references, predictions)]\n",
    "avg_bleu = np.mean(bleu_scores)\n",
    "\n",
    "print(\"Average BLEU Score:\", avg_bleu)\n",
    "\n",
    "# Log Rouge and BLEU scores to WandB\n",
    "wandb.log({\"rouge1\": rouge_scores['rouge1'].mid.fmeasure,\n",
    "           \"rouge2\": rouge_scores['rouge2'].mid.fmeasure,\n",
    "           \"rougeL\": rouge_scores['rougeL'].mid.fmeasure,\n",
    "           \"avg_bleu\": avg_bleu})\n",
    "\n",
    "\n",
    "# Close the wandb run\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
