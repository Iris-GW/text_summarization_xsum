{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from datasets import load_dataset\n",
    "from rouge import Rouge\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the XSum dataset\n",
    "dataset = load_dataset(\"xsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "def preprocess_batch(batch):\n",
    "    input_texts = [\"summarize: \" + doc for doc in batch[\"document\"]]\n",
    "    target_texts = batch[\"summary\"]\n",
    "\n",
    "    source = tokenizer(input_texts, max_length=512, truncation=True, padding='max_length', return_tensors=\"pt\")\n",
    "    target = tokenizer(target_texts, max_length=150, truncation=True, padding='max_length', return_tensors=\"pt\")\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": source[\"input_ids\"].tolist(),\n",
    "        \"attention_mask\": source[\"attention_mask\"].tolist(),\n",
    "        \"labels\": target[\"input_ids\"].tolist(),\n",
    "    }\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_batch, remove_columns=[\"document\", \"summary\"], batched=True, batch_size=16)\n",
    "train_dataset = tokenized_dataset[\"train\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha = 0.9\n",
    "gamma = 0.99\n",
    "n_epochs = 3\n",
    "batch_size = 8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "rouge = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Rouge-L score\n",
    "def compute_rouge_l(pred_summary, ref_summary):\n",
    "    scores = rouge.compute(predictions=[pred_summary], references=[ref_summary], rouge_types=[\"rougeL\"])\n",
    "    return scores[\"rougeL\"].fmeasure[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using PPO\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = 0\n",
    "    for i in range(0, len(train_dataset), batch_size):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch = train_dataset[i:i+batch_size]\n",
    "        input_ids = torch.stack(batch[\"input_ids\"]).to(device)\n",
    "        attention_mask = torch.stack(batch[\"attention_mask\"]).to(device)\n",
    "        labels = torch.stack(batch[\"labels\"]).to(device)\n",
    "\n",
    "        # Generate summaries\n",
    "        with torch.no_grad():\n",
    "            summary_ids = model.generate(input_ids, attention_mask=attention_mask, num_beams=4, max_length=150, early_stopping=True)\n",
    "        pred_summaries = [tokenizer.decode(s, skip_special_tokens=True) for s in summary_ids]\n",
    "        ref_summaries = [tokenizer.decode(s, skip_special_tokens=True) for s in labels]\n",
    "\n",
    "        # Calculate rewards\n",
    "        rewards = []\n",
    "        for pred_summary, ref_summary in zip(pred_summaries, ref_summaries):\n",
    "            reward = compute_rouge_l(pred_summary, ref_summary)\n",
    "            rewards.append(reward)\n",
    "\n",
    "        rewards = torch.tensor(rewards).to(device)\n",
    "\n",
    "        # Compute the policy gradient loss\n",
    "        logits = model(input_ids, attention_mask=attention_mask).logits\n",
    "        log_probs = torch.gather(logits.view(-1, logits.size(-1)), 1, labels.view(-1, 1)).view(batch_size, -1)\n",
    "        loss = -(rewards * log_probs).sum() / batch_size\n",
    "\n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} Loss: {epoch_loss / len(train_dataset):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "def generate_summary(text):\n",
    "    input_text = \"summarize: \" + text\n",
    "    inputs = tokenizer(input_text, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        summary_ids = model.generate(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], num_beams=4, max_length=150, early_stopping=True)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "rouge = Rouge()\n",
    "test_dataset = tokenized_dataset[\"test\"].select(range(1000))\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for example in test_dataset:\n",
    "    article = tokenizer.decode(example[\"input_ids\"], skip_special_tokens=True)\n",
    "    pred_summary = generate_summary(article)\n",
    "    ref_summary = tokenizer.decode(example[\"labels\"], skip_special_tokens=True)\n",
    "\n",
    "    predictions.append(pred_summary)\n",
    "    references.append(ref_summary)\n",
    "\n",
    "rouge_scores = rouge.compute(predictions=predictions, references=references, rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "\n",
    "print(\"Rouge Scores:\", rouge_scores)\n",
    "\n",
    "# Calculate BLEU scores\n",
    "smooth = SmoothingFunction().method1\n",
    "bleu_scores = [sentence_bleu([ref.split()], pred.split(), smoothing_function=smooth) for ref, pred in zip(references, predictions)]\n",
    "avg_bleu = np.mean(bleu_scores)\n",
    "\n",
    "print(\"Average BLEU Score:\", avg_bleu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
