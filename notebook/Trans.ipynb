{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, T5Config, TrainingArguments, Trainer\n",
    "import wandb\n",
    "import numpy as np\n",
    "from rouge import Rouge\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the XSum dataset\n",
    "dataset = load_dataset(\"xsum\")\n",
    "\n",
    "# Reduce dataset size\n",
    "train_indices, test_data = train_test_split(range(len(dataset['train'])), test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"text_summarization\", name=\"transformer_summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model name and tokenizer\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(batch):\n",
    "    input_texts = [\"summarize: \" + doc for doc in batch[\"document\"]]\n",
    "    target_texts = batch[\"summary\"]\n",
    "\n",
    "    source = tokenizer(input_texts, max_length=512, truncation=True, padding='max_length', return_tensors=\"pt\")\n",
    "    target = tokenizer(target_texts, max_length=150, truncation=True, padding='max_length', return_tensors=\"pt\")\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": source[\"input_ids\"].tolist(),\n",
    "        \"attention_mask\": source[\"attention_mask\"].tolist(),\n",
    "        \"labels\": target[\"input_ids\"].tolist(),\n",
    "    }\n",
    "\n",
    "# Replace the original train dataset with the reduced dataset\n",
    "reduced_dataset = {}\n",
    "reduced_dataset[\"train\"] = dataset[\"train\"].select(train_indices)\n",
    "reduced_dataset[\"validation\"] = dataset[\"validation\"]\n",
    "reduced_dataset[\"test\"] = dataset[\"test\"]\n",
    "\n",
    "# Preprocess the data\n",
    "tokenized_dataset = {}\n",
    "for split in reduced_dataset:\n",
    "    tokenized_dataset[split] = reduced_dataset[split].map(preprocess_batch, remove_columns=[\"document\", \"summary\"], batched=True, batch_size=16)\n",
    "\n",
    "train_dataset = tokenized_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the T5 model\n",
    "config = T5Config.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training and evaluation arguments\n",
    "num_train_examples = len(train_dataset)\n",
    "train_batch_size = 8\n",
    "steps_per_epoch = len(reduced_dataset[\"train\"]) // train_batch_size\n",
    "save_steps = int(steps_per_epoch * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5_xsum\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1275,  # Change the eval_steps to be a multiple of save_steps\n",
    "    logging_dir=\"./logs\",\n",
    "    weight_decay=0.01,\n",
    "    save_steps=save_steps,\n",
    "    save_total_limit=3,  # Limit the number of saved checkpoints\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"wandb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_dir = \"./t5_xsum\"\n",
    "\n",
    "# Get all checkpoint filenames\n",
    "checkpoint_filenames = os.listdir(checkpoint_dir)\n",
    "\n",
    "# Filter filenames to get only checkpoint files\n",
    "checkpoint_filenames = [filename for filename in checkpoint_filenames if \"checkpoint\" in filename]\n",
    "\n",
    "# Sort filenames by epoch number\n",
    "checkpoint_filenames = sorted(checkpoint_filenames, key=lambda x: float(x.split(\"-\")[-1]))\n",
    "\n",
    "# Print the filename with the largest epoch number\n",
    "print(checkpoint_filenames[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint load\n",
    "checkpoint_path = \"./t5_xsum/checkpoint-3825\"\n",
    "\n",
    "# Check if the checkpoint is loadable\n",
    "try:\n",
    "    model_test = T5ForConditionalGeneration.from_pretrained(checkpoint_path)\n",
    "    print(\"Checkpoint is loadable.\")\n",
    "except Exception as e:\n",
    "    print(\"Error loading checkpoint:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train(\"./t5_xsum/checkpoint-36975\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainer.state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"./t5_xsum_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Reduce the test dataset to 0.02\n",
    "_, reduced_test_indices = train_test_split(test_data, test_size=0.02, random_state=42)\n",
    "reduced_test_data = [dataset[\"train\"][i] for i in reduced_test_indices]\n",
    "\n",
    "def generate_summary_transformer(article):\n",
    "    model = T5ForConditionalGeneration.from_pretrained('./t5_xsum_finetuned')\n",
    "    tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "    inputs = tokenizer.encode(\"summarize: \" + article, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    summary_ids = model.generate(inputs, num_return_sequences=1, max_length=150, no_repeat_ngram_size=2, min_length=30, early_stopping=True)\n",
    "\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "def evaluate_model(generate_summary_function, reduced_test_data):\n",
    "    rouge = Rouge()\n",
    "    bleu_score = 0\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for example in reduced_test_data:\n",
    "        article = example[\"document\"]\n",
    "        summary = generate_summary_function(article)\n",
    "        predictions.append(summary)\n",
    "        references.append(example[\"summary\"])\n",
    "\n",
    "    rouge_scores = rouge.get_scores(predictions, references, avg=True)\n",
    "\n",
    "    # Compute BLEU score\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        pred_tokens = pred.split()\n",
    "        ref_tokens = ref.split()\n",
    "        bleu_score += sentence_bleu([ref_tokens], pred_tokens, smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "    bleu_score = bleu_score / len(predictions)\n",
    "\n",
    "    return rouge_scores, bleu_score\n",
    "\n",
    "\n",
    "# Evaluate the Transformer-based model\n",
    "transformer_rouge_scores, transformer_bleu_score = evaluate_model(generate_summary_transformer, reduced_test_data)\n",
    "print(\"Transformer Rouge Scores:\", transformer_rouge_scores)\n",
    "print(\"Transformer BLEU Score:\", transformer_bleu_score)\n",
    "\n",
    "# Log Rouge and BLEU scores to WandB\n",
    "wandb.log({\"rouge1\": transformer_rouge_scores['rouge-1']['f'],\n",
    "           \"rouge2\": transformer_rouge_scores['rouge-2']['f'],\n",
    "           \"rougeL\": transformer_rouge_scores['rouge-l']['f'],\n",
    "           \"avg_bleu\": transformer_bleu_score})\n",
    "\n",
    "# Finish the run\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
